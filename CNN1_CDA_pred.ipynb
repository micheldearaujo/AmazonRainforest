{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo com data augmentation  para o treinamento de uma rede neural convolucional\n",
    "# Com dados de classificação multi-label\n",
    "\n",
    "# Importanto as bibliotecas necessárias\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho dos diretorios\n",
    "#base_dir = '/home/michel/data/amazonia/kaggle' # Ubuntu\n",
    "base_dir = 'D:/michel/data/amazonia/kaggle' # Windows\n",
    "train_dir = os.path.join(base_dir, 'train-jpg')\n",
    "test_dir = os.path.join(base_dir, 'test-jpg')\n",
    "train_fnames = os.listdir(train_dir)\n",
    "test_fnames = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dimensão do dataframe é:  (40479, 2)\n"
     ]
    }
   ],
   "source": [
    "# Definindo qual é o dataset que usaremos e qual o optimizer\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "targ_shape = (16,16,3)\n",
    "targ_size = targ_shape[:-1]\n",
    "dataset_name = 'amazon_data_%s.npz'%(targ_shape[0])\n",
    "model_name = 'CNN1_CDA_%s_adam.h5'%(targ_shape[0])\n",
    "mapping_csv = pd.read_csv(base_dir + '/train_classes.csv')\n",
    "print(\"A dimensão do dataframe é: \",mapping_csv.shape) # Dimensões do dataframe com os labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dados que ja criamos no 'create_dataset.py'\n",
    "def load_testset(dataset_name):\n",
    "    # Carregando\n",
    "    data = np.load(base_dir + '/'+ dataset_name)\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "    # Separando os sets de training e testing\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    Xte, yte = Xte[:4048,:], yte[:4048]\n",
    "    print('As dimensões dos vetores são: \\n')\n",
    "    print('Xte shape: ', Xte.shape)\n",
    "    print('\\n')\n",
    "    print('yte shape: ', yte.shape)\n",
    "    print('\\n')\n",
    "    return Xte, yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função para calcular o fbeta score\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    # Clipando a previsao\n",
    "    y_pred = backend.clip(y_pred, 0, 1)\n",
    "    tp = backend.sum(backend.round(backend.clip(y_true*y_pred, 0, 1)), axis=1)\n",
    "    fp = backend.sum(backend.round(backend.clip(y_pred-y_true, 0, 1)), axis=1)\n",
    "    fn = backend.sum(backend.round(backend.clip(y_true-y_pred, 0, 1)), axis=1)\n",
    "    # Calculando a precisao\n",
    "    p = tp/(tp+fp+backend.epsilon())\n",
    "    # Calculando o Recall\n",
    "    r = tp/(tp+fn+backend.epsilon())\n",
    "    # calculando o fbeta, tirado a média para cada classe\n",
    "    bb = beta**2\n",
    "    fbeta_score = backend.mean((1+bb)*(p*r)/(bb*p+r+backend.epsilon()))\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando a imagem de test\n",
    "img_name = 'train_40477.jpg'\n",
    "img = load_img(train_dir+'/'+img_name, target_size=targ_size)\n",
    "imgarray = img_to_array(img)\n",
    "imgarray = imgarray.reshape((1,)+imgarray.shape) # Alterando a dimensão, agora é um vetor unidimensional\n",
    "imgarray = imgarray/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_map(mapping_csv):\n",
    "    labels = set()\n",
    "    for i in range(len(mapping_csv)):\n",
    "        tags = mapping_csv['tags'][i].split(' ')\n",
    "        labels.update(tags)\n",
    "\n",
    "    labels = list(labels)\n",
    "    labels.sort()\n",
    "    labels_map = {labels[k]: k for k in range(len(labels))}\n",
    "    inv_labels_map = {k: labels[k] for k in range(len(labels))}\n",
    "    return labels_map, inv_labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_mapping(mapping_csv):\n",
    "    mapping=dict() # Criamos um dicionário vazio\n",
    "    for j in range(len(mapping_csv)):\n",
    "        \"\"\" percorremos o dataframe inteiro, pegando cada nome da imagem\n",
    "        e sua respectiva tag, então separamos a tag por espaço\n",
    "        e dizemos que o nome da tag é igual a sua tag, isso cria o dicionário!\n",
    "        Agora temos multilabels para cada imagem.\n",
    "        \"\"\"\n",
    "        name, tags = mapping_csv['image_name'][j], mapping_csv['tags'][j]\n",
    "        mapping[name] = tags.split(' ')\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_enconde(tags, mapping):\n",
    "    enconding = np.zeros(len(mapping), dtype='uint8')\n",
    "    for tag in tags:\n",
    "        enconding[mapping[tag]] = 1\n",
    "    return enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map, inv_labels_map = create_tag_map(mapping_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'agriculture',\n",
       " 1: 'artisinal_mine',\n",
       " 2: 'bare_ground',\n",
       " 3: 'blooming',\n",
       " 4: 'blow_down',\n",
       " 5: 'clear',\n",
       " 6: 'cloudy',\n",
       " 7: 'conventional_mine',\n",
       " 8: 'cultivation',\n",
       " 9: 'habitation',\n",
       " 10: 'haze',\n",
       " 11: 'partly_cloudy',\n",
       " 12: 'primary',\n",
       " 13: 'road',\n",
       " 14: 'selective_logging',\n",
       " 15: 'slash_burn',\n",
       " 16: 'water'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-81317bc5c347>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Carregando o modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodelo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfbeta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m       if (h5py is not None and\n\u001b[0;32m    205\u001b[0m           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 206\u001b[1;33m         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[0;32m    207\u001b[0m                                                 compile)\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m    184\u001b[0m                                                custom_objects=custom_objects)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# Carregando o modelo\n",
    "modelo = load_model(base_dir+'/'+model_name, compile=False)\n",
    "modelo.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7407e2146318>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prevendo uma classe:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msingle_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo' is not defined"
     ]
    }
   ],
   "source": [
    "# Prevendo uma classe:\n",
    "single_prediction = modelo.predict_classes(imgarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b560f7703204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prevendo as probabilidades de cada classe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmulti_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'modelo' is not defined"
     ]
    }
   ],
   "source": [
    "# Prevendo as probabilidades de cada classe\n",
    "multi_prediction = modelo.predict_proba(imagarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
